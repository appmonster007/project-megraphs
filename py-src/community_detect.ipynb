{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing required libraries for creating graph object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.io import mmread\n",
    "from Base import Graph\n",
    "from IPython.display import display\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialising and creating instances of graph object using different *.mtx files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# karate = mmread('soc-karate.mtx')\n",
    "# webedu = mmread('web-edu.mtx')\n",
    "# internet = mmread('tech-internet-as.mtx')\n",
    "\n",
    "karate = mmread('../assets/S_soc-karate.mtx')\n",
    "webedu = mmread('../assets/M_web-edu.mtx')\n",
    "internet = mmread('../assets/L_tech-internet-as.mtx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "G1 = Graph(mtxfilepath='../assets/S_soc-karate.mtx')\n",
    "G2 = Graph(sparse=webedu)\n",
    "G3 = Graph(sparse=internet)\n",
    "print((\"-\"*50)+\"Graphs objects created\"+(\"-\"*50))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graphData = [['soc-karate.mtx', G1.graph.size(), G1.is_connected()],\n",
    "            ['web-edu.mtx', G2.graph.size(), G2.is_connected()],\n",
    "            ['tech-internet-as.mtx', G3.graph.size(), G3.is_connected()]]\n",
    "            \n",
    "display(pd.DataFrame(graphData, columns=[\"Name\", \"Size\", \"connected\"]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finding Centralities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EGO centrality\n",
    "# print(G.ego_centrality_node(4))\n",
    "# print(\"ego graph made\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finding lfvc node\n",
    "\n",
    "lfvc1 = G1.lfvc_node(0)\n",
    "lfvc2 = G2.lfvc_node(0)\n",
    "# lfvc3 = G3.lfvc_node(0)\n",
    "print(lfvc1)\n",
    "print(lfvc2)\n",
    "# print(lfvc3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finding nodes of interest\n",
    "graphData = [['soc-karate.mtx', G1.nodes_of_interest()],\n",
    "            ['web-edu.mtx', G2.nodes_of_interest()],\n",
    "            ['tech-internet-as.mtx', G3.nodes_of_interest()]]\n",
    "display(pd.DataFrame(graphData, columns=[\"Name\", \"Nodes of interest: \"]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finding Centralities of smallest size graph, i.e. soc-karate\n",
    "\n",
    "print(\"soc-karate :\")\n",
    "dc1 = G1.degree_centrality()\n",
    "cc1 = G1.closeness_centrality()\n",
    "bc1 = G1.betweenness_centrality()\n",
    "ec1 = G1.eigenvector_centrality()\n",
    "clc1 = G1.clustering_coefficient_node(0)\n",
    "lfvc_val = G1.lfvc()\n",
    "nhc1 = G1.neighbourhood_hopset(0,2)\n",
    "\n",
    "\n",
    "data = [['lfvc', lfvc_val],\n",
    "        ['degree centrality', dc1],\n",
    "        ['closeness centrality', cc1],\n",
    "        ['betweenness centrality', bc1],\n",
    "        ['eigenvector centrality', ec1],\n",
    "        ['neighbouring hopset', nhc1],\n",
    "        ['Clusters of node 1', clc1]]\n",
    "\n",
    "display(pd.DataFrame(data, columns=[\"Centrality\", \"value\"]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finding nodes of interest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nodes_interest1 = G1.nodes_of_interest()\n",
    "nodes_interest2 = G2.nodes_of_interest()\n",
    "nodes_interest3 = G3.nodes_of_interest()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Centralities at nodes of interest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finding Centralities of medium size graph, i.e. web-edu\n",
    "\n",
    "print(\"web-edu :\")\n",
    "for i in nodes_interest2:\n",
    "    print(\"\\nNode \", i)\n",
    "    cc2 = G2.closeness_centrality_node(i)\n",
    "    clc2 = G2.clustering_coefficient_node(i)\n",
    "    ec2 = G2.ego_centrality_node(i)\n",
    "    lfvc_val2 = G2.lfvc_node(i)\n",
    "    nhc2 = G2.neighbourhood_hopset(i,2)\n",
    "    eig_c2 = G2.eigenvector_centrality_node(i)\n",
    "\n",
    "    data = [['lfvc', lfvc_val2],\n",
    "        ['closeness centrality', cc2],\n",
    "        ['Clusters of node 1', clc2],\n",
    "        ['neighbouring hopset', nhc2],\n",
    "        ['ego centrality', ec2],\n",
    "        ['eigenvector centrality', eig_c2]]\n",
    "\n",
    "    display(pd.DataFrame(data, columns=[\"Centrality\", \"value\"]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finding Centralities of largest size graph, i.e. tech-internet-as\n",
    "\n",
    "print(\"tech-internet-as :\")\n",
    "for i in nodes_interest3:\n",
    "    print(\"\\nNode \", i)\n",
    "    cc3 = G3.closeness_centrality_node(i)\n",
    "    clc3 = G3.clustering_coefficient_node(i)\n",
    "    ec3 = G3.ego_centrality_node(i)\n",
    "    lfvc_val3 = G3.lfvc_node(i)\n",
    "    nhc3 = G3.neighbourhood_hopset(i,2)\n",
    "    eig_c3 = G3.eigenvector_centrality_node(i)\n",
    "\n",
    "    data = [['lfvc', lfvc_val3],\n",
    "        ['closeness centrality', cc3],\n",
    "        ['Clusters of node 1', clc3],\n",
    "        ['neighbouring hopset', nhc3],\n",
    "        ['ego centrality', ec3],\n",
    "        ['eigenvector centrality', eig_c3]]\n",
    "\n",
    "    display(pd.DataFrame(data, columns=[\"Centrality\", \"value\"]))\n",
    "\n",
    "\n",
    "\n",
    "# dc3 = G3.degree_centrality()\n",
    "# cc3 = G3.closeness_centrality()\n",
    "# bc3 = G3.betweenness_centrality()\n",
    "# eig_c3 = G3.eigenvector_centrality()\n",
    "# clc3 = G3.clustering_coefficient_node(0)\n",
    "# lfvc_val3 = G3.lfvc_node(0)\n",
    "# nhc3 = G3.neighbourhood_hopset(0,2)\n",
    "# print((\"-\"*100))\n",
    "# print(\"lfvc\")\n",
    "# print(lfvc_val3)\n",
    "\n",
    "# data = [[1, 'lfvc', lfvc_val3],\n",
    "#         [2, 'degree centrality', len(dc3)],\n",
    "#         [3, 'closeness centrality', len(cc3)],\n",
    "#         [4, 'betweenness centrality', len(bc3)],\n",
    "#         [5, 'eigenvector centrality', len(eig_c3)],\n",
    "#         [6, 'neighbouring hopset', nhc3],\n",
    "#         [7, 'Clusters of node 1', clc3]]\n",
    "\n",
    "# print(tabulate(data, headers=[\"#\", \"Centrality\", \"len of array/value\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep community detection using greedy lfvc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dc = G1.cd_g_lfvc(5)\n",
    "print(dc)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
  },
  "kernelspec": {
   "display_name": "Python 3.9.5 64-bit",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
